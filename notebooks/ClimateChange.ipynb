{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.StreamProcessing import Stream_Data # found in the scripts folder inside StreamProcessing.py. This class uses PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://berkeleyearth.org/data/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on block \n",
    "\n",
    "These might take a while as PySpark attempts to download some additional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 19:06:49 WARN Utils: Your hostname, camagakhan-Inspiron-7577 resolves to a loopback address: 127.0.1.1; using 192.168.6.63 instead (on interface wlp60s0)\n",
      "23/05/14 19:06:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/camagakhan/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/camagakhan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/camagakhan/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8cc132f1-4f50-4df2-a459-6089bf29216e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 713ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8cc132f1-4f50-4df2-a459-6089bf29216e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/13ms)\n",
      "23/05/14 19:06:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Stream_Data Class using the topic name/s established in the kafka-config/kafka-config.ipynb and add the host of your kafka instance in the host parameter\n",
    "emissions_config = Stream_Data(topics='ghg_data', host='localhost:9092')\n",
    "query, emissions_df = emissions_config.getData()\n",
    "#temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/mysql-connector-j-8.0.33.jar\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Stream_Data Class using the topic name/s established in the kafka-config/kafka-config.ipynb and add the host of your kafka instance in the host parameter\n",
    "temperature_config = Stream_Data(topics='temperature', host='localhost:9092')\n",
    "query_temperature, temperature_df = temperature_config.getData()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Everything is configered while initializing the PySpark Session in the ```StreamProcessing.py``` class. DO NOT remove the ```from graphframes import *```, as the jar files are loaded while getting the data from the previous code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark found (0 + 1) / 1][Stage 1:>    (0 + 1) / 1][Stage 2:>    (0 + 1) / 1]1]\n",
      "PySpark found\n",
      "23/05/14 19:07:02 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)    \n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.close(ForeachWriterTable.scala:166)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1585)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/05/14 19:07:02 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "23/05/14 19:07:02 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@17e7fefb] is aborting.\n",
      "23/05/14 19:07:02 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@17e7fefb] aborted.\n",
      "23/05/14 19:07:02 ERROR MicroBatchExecution: Query [id = 493fffdf-e949-4844-a4d5-beaf2c857c53, runId = fb80bdc6-3465-48dc-89ea-3b4332592ae0] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=3, partition=0) failed; but task commit success, data duplication may happen. reason=ExceptionFailure(org.apache.spark.api.python.PythonException,Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      ",[Ljava.lang.StackTraceElement;@67835f36,org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.close(ForeachWriterTable.scala:166)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1585)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      ",Some(org.apache.spark.ThrowableSerializationWrapper@5838ee70),Vector(AccumulableInfo(124,None,Some(2831),None,false,true,None), AccumulableInfo(126,None,Some(0),None,false,true,None), AccumulableInfo(127,None,Some(133),None,false,true,None), AccumulableInfo(154,None,Some(30014),None,false,true,None)),Vector(LongAccumulator(id: 124, name: Some(internal.metrics.executorRunTime), value: 2831), LongAccumulator(id: 126, name: Some(internal.metrics.resultSize), value: 0), LongAccumulator(id: 127, name: Some(internal.metrics.jvmGCTime), value: 133), LongAccumulator(id: 154, name: Some(internal.metrics.input.recordsRead), value: 30014)),WrappedArray(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1(DAGScheduler.scala:1199)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1$adapted(DAGScheduler.scala:1199)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleStageFailed(DAGScheduler.scala:1199)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2981)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:738)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                  (0 + 1) / 1][Stage 2:>                  (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+-----------+---------+--------+\n",
      "|REF_AREA|             Measure|      UNIT_MEASURE|TIME_PERIOD|OBS_VALUE|REF_CODE|\n",
      "+--------+--------------------+------------------+-----------+---------+--------+\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1979|    -0.55|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1980|    -0.17|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1981|    -0.48|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1982|    -0.53|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1983|    -0.89|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1984|    -0.82|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1985|    -0.43|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1986|    -0.53|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1987|    -0.12|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1988|    -0.14|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1989|    -0.64|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1990|     0.31|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1991|     0.05|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1992|    -0.59|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1993|    -0.28|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1994|    -0.02|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1995|    -0.09|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1996|    -0.17|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1997|     0.36|     YEM|\n",
      "|   Yemen|TEMP_C: Annual te...|C: Degrees celsius|       1998|     0.64|     YEM|\n",
      "+--------+--------------------+------------------+-----------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/14 19:07:06 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.close(ForeachWriterTable.scala:166)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1585)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/05/14 19:07:06 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n",
      "23/05/14 19:07:06 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$anon$2@4bd2d109] is aborting.\n",
      "23/05/14 19:07:06 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@4bd2d109] aborted.\n",
      "23/05/14 19:07:06 ERROR MicroBatchExecution: Query [id = 18e46c79-70c3-4809-9ce1-54d11193f93a, runId = 1afe2d41-c9fe-4a1e-916e-0b71154c3eb3] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (192.168.6.63 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.close(ForeachWriterTable.scala:166)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1585)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:341)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3418)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:738)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:726)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/camagakhan/spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\", line 1194, in func_without_process\n",
      "    f(x)  # type: ignore[operator]\n",
      "  File \"/media/camagakhan/DATA/Repositories/BigDataProcessing/Assignment/BigDataProcessingClimateChange/notebooks/scripts/StreamProcessing.py\", line 194, in __write__\n",
      "    conn = mysql.connector.connect(**sqlConfiguration)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/pooling.py\", line 293, in connect\n",
      "    return CMySQLConnection(*args, **kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/connection_cext.py\", line 120, in __init__\n",
      "    self.connect(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 1178, in connect\n",
      "    self.config(**kwargs)\n",
      "  File \"/home/camagakhan/anaconda3/lib/python3.9/site-packages/mysql/connector/abstracts.py\", line 627, in config\n",
      "    raise AttributeError(f\"Unsupported argument '{key}'\") from None\n",
      "AttributeError: Unsupported argument 'connectTimeout'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.PythonForeachWriter.close(PythonForeachWriter.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachDataWriter.close(ForeachWriterTable.scala:166)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1585)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+---------+---+----------------+-----+--------------------+----+----+----------+--------------------+-------------+---------+-------------------+---------------+----------+---------+-----+\n",
      "|﻿\"COU\"|  Country|POL|       Pollutant|  VAR|            Variable| YEA|Year|  UnitCode|                Unit|PowerCodeCode|PowerCode|ReferencePeriodCode|ReferencePeriod|     Value|FlagCodes|Flags|\n",
      "+------+---------+---+----------------+-----+--------------------+----+----+----------+--------------------+-------------+---------+-------------------+---------------+----------+---------+-----+\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1990|1990|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|425624.307|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1991|1991|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|425686.445|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1992|1992|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|429473.085|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1993|1993|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|430381.731|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1994|1994|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|430848.498|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1995|1995|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|439269.878|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1996|1996|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|445847.225|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1997|1997|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null| 457864.51|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1998|1998|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|472145.832|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|1999|1999|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|478136.875|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2000|2000|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|489528.649|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2001|2001|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|497352.025|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2002|2002|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|501000.843|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2003|2003|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|501128.976|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2004|2004|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|518408.438|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2005|2005|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|524811.845|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2006|2006|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|529240.641|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2007|2007|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|535807.552|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2008|2008|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|538590.714|         |     |\n",
      "|   AUS|Australia|GHG|Greenhouse gases|TOTAL|Total  emissions ...|2009|2009|T_CO2_EQVT|Tonnes of CO2 equ...|            3|Thousands|               null|           null|541475.792|         |     |\n",
      "+------+---------+---+----------------+-----+--------------------+----+----+----------+--------------------+-------------+---------+-------------------+---------------+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from graphframes import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
